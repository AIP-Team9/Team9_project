{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thZm_fegUczD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uFPmiNCnVNyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_seoul = pd.read_csv('/content/drive/MyDrive/train_seoul.csv', encoding='utf-8')\n",
        "test_seoul = pd.read_csv('/content/drive/MyDrive/test_seoul.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "5X3MKKl2VVu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "target_var = \"관측미세먼지\"\n",
        "\n",
        "X_train = train_seoul.drop(columns=[target_var]).values\n",
        "Y_train = train_seoul[target_var].values\n",
        "\n",
        "X_test = test_seoul.drop(columns=[target_var]).values\n",
        "Y_test = test_seoul[target_var].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "columns_to_scale_train = np.hstack((X_train[:, :-3], X_train[:, -1:]))\n",
        "columns_to_scale_test = np.hstack((X_test[:, :-3], X_test[:, -1:]))\n",
        "X_train_scaled = scaler.fit_transform(columns_to_scale_train)\n",
        "X_test_scaled = scaler.transform(columns_to_scale_test)\n",
        "\n",
        "X_train = np.hstack((X_train_scaled, X_train[:, -3:-2], X_train[:, -2:-1]))\n",
        "X_test = np.hstack((X_test_scaled, X_test[:, -3:-2], X_test[:, -2:-1]))\n",
        "\n",
        "train_columns = list(train_seoul.columns[:-4]) + [train_seoul.columns[-1], train_seoul.columns[-4], train_seoul.columns[-3]]\n",
        "\n",
        "scaled_train_seoul = pd.DataFrame(X_train, columns=train_columns)\n",
        "scaled_train_seoul[target_var] = Y_train\n",
        "scaled_test_seoul = pd.DataFrame(X_test, columns=train_columns)\n",
        "scaled_test_seoul[target_var] = Y_test"
      ],
      "metadata": {
        "collapsed": true,
        "id": "euEHFtx3WNRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, sequence_length=1440):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    # \"경과일\"과 \"경과시간\"을 분 단위로 변환\n",
        "    data['total_minutes'] = data['경과일'] * 1440 + data['경과시간']\n",
        "\n",
        "    start_index = 0\n",
        "\n",
        "    while start_index < len(data):\n",
        "        start_time = data.iloc[start_index]['total_minutes']\n",
        "        end_time = start_time + sequence_length\n",
        "\n",
        "        end_index = start_index\n",
        "        while end_index < len(data) and data.iloc[end_index]['total_minutes'] < end_time:\n",
        "            end_index += 1\n",
        "\n",
        "        subset = data.iloc[start_index:end_index]\n",
        "        if len(subset) > 0:\n",
        "            sequence = subset.drop(columns=['경과일', '경과시간', 'total_minutes', '관측미세먼지']).values\n",
        "\n",
        "            # 시퀀스가 너무 길면 자르기\n",
        "            if len(sequence) > sequence_length:\n",
        "                sequence = sequence[:sequence_length]\n",
        "            else:\n",
        "                # 시퀀스 길이를 일정하게 만들기 위해 padding 적용\n",
        "                padded_sequence = np.zeros((sequence_length, sequence.shape[1]))\n",
        "                padded_sequence[:sequence.shape[0], :] = sequence\n",
        "                sequence = padded_sequence\n",
        "\n",
        "            sequences.append(sequence)\n",
        "            # 라벨 생성: 마지막 \"관측미세먼지\" 값을 라벨로 사용\n",
        "            label = 1 if subset[target_var].iloc[-1] > 80 else 0  # 라벨을 이진 분류 문제로 설정\n",
        "            labels.append(label)\n",
        "\n",
        "        start_index = end_index\n",
        "\n",
        "    return np.array(sequences), np.array(labels)"
      ],
      "metadata": {
        "id": "HBBu5h8ViZfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시계열 데이터 생성\n",
        "X_train_seq, y_train_seq = create_sequences(scaled_train_seoul)\n",
        "X_test_seq, y_test_seq = create_sequences(scaled_test_seoul)"
      ],
      "metadata": {
        "id": "gkW7j3YQqraK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X_train_seq, y_train_seq)\n",
        "test_dataset = TimeSeriesDataset(X_test_seq, y_test_seq)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "q4D-_rhUkbOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
        "        super(TransformerEncoderModel, self).__init__()\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_encoder_layers)\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, input_dim)\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = output.mean(dim=0)  # Average over the sequence length\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "input_dim = 28  # input_dim을 28로 설정\n",
        "num_classes = 2\n",
        "nhead = 4\n",
        "num_encoder_layers = 3\n",
        "dim_feedforward = 128\n",
        "dropout = 0.1\n",
        "\n",
        "model = TransformerEncoderModel(input_dim, num_classes, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "el5THS1Am7s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cross validation"
      ],
      "metadata": {
        "id": "XEmnRdwI-cCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# K-fold Cross Validation을 위한 KFold 객체 생성\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 각 폴드에서의 평가 지표 저장을 위한 리스트 선언\n",
        "accuracies = []\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X_train_seq, y_train_seq)):\n",
        "    print(f'Fold [{fold + 1}/{kf.get_n_splits()}]')\n",
        "\n",
        "    # 해당 폴드를 위한 학습 및 검증 데이터 분할\n",
        "    X_fold_train, X_fold_val = X_train_seq[train_index], X_train_seq[val_index]\n",
        "    y_fold_train, y_fold_val = y_train_seq[train_index], y_train_seq[val_index]\n",
        "\n",
        "    # DataLoader 정의\n",
        "    fold_train_dataset = TimeSeriesDataset(X_fold_train, y_fold_train)\n",
        "    fold_val_dataset = TimeSeriesDataset(X_fold_val, y_fold_val)\n",
        "    fold_train_loader = DataLoader(fold_train_dataset, batch_size=32, shuffle=True)\n",
        "    fold_val_loader = DataLoader(fold_val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = TransformerEncoderModel(input_dim, num_classes, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
        "\n",
        "    # 옵티마이저 재설정\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    num_epochs = 2\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for X_batch, y_batch in fold_train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Test loop\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in fold_val_loader:\n",
        "            output = model(X_batch)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    fold_accuracy = 100 * correct / total\n",
        "    accuracies.append(fold_accuracy)\n",
        "    print(f'Fold [{fold + 1}/{kf.get_n_splits()}], Accuracy: {fold_accuracy:.2f}%')\n",
        "\n",
        "# K-fold Cross Validation의 평균 Accuracy 계산\n",
        "avg_accuracy = np.mean(accuracies)\n",
        "print(f'Average Accuracy: {avg_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "uuRNsrzS-bjr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}