{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!apt-get -qq install fonts-nanum\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib as mpl\n",
        "\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "fm.fontManager.addfont(fontpath)\n",
        "mpl.rc('font', family='NanumGothic')\n",
        "mpl.rcParams['axes.unicode_minus'] = False\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/data_2024_3.csv', encoding='euc-kr')\n",
        "\n",
        "# Shuffle the data\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(data.head())\n",
        "\n",
        "\n",
        "# Define split ratios\n",
        "#train_ratio = 0.7\n",
        "#validation_ratio = 0.15\n",
        "#test_ratio = 0.15\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "#train_data, temp_data = train_test_split(data, test_size=(1 - train_ratio))\n",
        "#val_data, test_data = train_test_split(temp_data, test_size=test_ratio/(test_ratio + validation_ratio))\n",
        "\n",
        "# Save the splits to CSV files if needed\n",
        "#train_data.to_csv(\"train_data.csv\", index=False)\n",
        "#val_data.to_csv(\"val_data.csv\", index=False)\n",
        "#test_data.to_csv(\"test_data.csv\", index=False)\n",
        "\n",
        "data.dropna(subset=[\"관측시간\", \"관측지점\"], inplace=True)\n",
        "\n",
        "# 관측시간을 datetime 타입으로 변환\n",
        "data[\"관측시간\"] = pd.to_datetime(data[\"관측시간\"])\n",
        "\n",
        "data[[\"관측지점\", \"관측지점세부\"]] = data[\"관측지점\"].str.split('_', expand=True)\n",
        "\n",
        "columns_to_fill = [\"관측최대풍속\"]\n",
        "columns_to_fill = [\"관측최대풍속\", \"(AVOC)관측최대풍속\", \"(BVOC)관측최대풍속\"]\n",
        "\n",
        "for column in columns_to_fill:\n",
        "    data[column] = data.groupby([\"관측시간\", \"관측지점\"])[column].transform(\n",
        "        lambda x: x.fillna(x.mean())\n",
        "    )\n",
        "\n",
        "    # 그래도 남아있는 결측치는 관측지점으로 채우기\n",
        "    data[column] = data.groupby('관측지점')[column].transform(\n",
        "        lambda x: x.fillna(x.mean())\n",
        "    )\n",
        "\n",
        "    # 그래도 남아있는 결측치는 관측시간으로 채우기\n",
        "    data[column] = data.groupby('관측시간')[column].transform(\n",
        "        lambda x: x.fillna(x.mean())\n",
        "    )\n",
        "\n",
        "    # 여전히 남아있는 결측치는 전체 평균으로 채우기\n",
        "    overall_mean = data[column].mean()\n",
        "    data[column].fillna(overall_mean, inplace=True)\n",
        "\n",
        "    # 결측치 처리 전 결측치 확인\n",
        "if data.isnull().values.any():\n",
        "    print(\"데이터프레임에 결측치가 있습니다.\")\n",
        "else:\n",
        "    print(\"데이터프레임에 결측치가 없습니다.\")\n",
        "\n",
        "# 각 열의 결측치 수 확인\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"각 열의 결측치 수:\")\n",
        "print(missing_values)\n",
        "\n",
        "data.sort_values(by=\"관측시간\", inplace=True)\n",
        "\n",
        "train_ratio = 0.9\n",
        "\n",
        "train_size = int(len(data) * train_ratio)\n",
        "train_data = data.iloc[:train_size]\n",
        "test_data = data.iloc[train_size:]\n",
        "\n",
        "print(\"훈련 데이터 크기:\", len(train_data))\n",
        "print(\"테스트 데이터 크기:\", len(test_data))\n",
        "\n",
        "# 아직 관측지점 및 관측지점세부 고려하지 않음\n",
        "train_data = train_data.drop(columns=['관측지점', '관측지점세부'])\n",
        "test_data = test_data.drop(columns=['관측지점', '관측지점세부'])\n",
        "\n",
        "train_data.to_csv(\"train_data.csv\", index=False)\n",
        "test_data.to_csv(\"test_data.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDFRrJ99hGyc",
        "outputId": "575d963a-4f63-4b40-aa8f-3ff7eee5e5fc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "        관측지점              관측시간  관측온도  관측습도    관측기압  관측풍속   관측풍향  관측최대풍속  \\\n",
            "0       태안_숲  2024-03-24 18:40  14.7  47.7  1010.4   0.6  272.8     NaN   \n",
            "1     평창_30m  2024-03-26 20:30   0.3  83.7   911.4   0.5  208.2     NaN   \n",
            "2  삼척-동해_주거2  2024-03-22 23:30   6.8  93.5  1007.1   0.4  280.5     1.3   \n",
            "3  삼척-동해_주거1  2024-03-11 06:00   4.0  46.7  1017.8   0.2  264.7     0.5   \n",
            "4      춘천_산림  2024-03-18 07:40  -2.2  64.2   998.0   0.5   35.2     1.2   \n",
            "\n",
            "   관측미세먼지  관측초미세먼지  ...  (AVOC)배관관측온도  (BVOC)관측온도  (BVOC)관측습도  (BVOC)관측기압  \\\n",
            "0    41.2     25.4  ...         109.3        14.5        47.9      1010.4   \n",
            "1     3.0      2.8  ...         107.7         0.3        83.7       911.4   \n",
            "2    38.2     34.0  ...         101.6         6.8        93.5      1007.1   \n",
            "3    29.9     22.6  ...         105.0         4.0        46.5      1017.9   \n",
            "4    12.2      8.2  ...         101.3        -2.2        64.2       998.0   \n",
            "\n",
            "   (BVOC)관측풍속  (BVOC)관측풍향  (BVOC)관측최대풍속  (BVOC)관측미세먼지  (BVOC)관측초미세먼지  \\\n",
            "0         0.6       282.9           NaN          13.8            5.8   \n",
            "1         0.7       192.1           NaN           0.9            0.9   \n",
            "2         0.4       288.0           1.8           4.4            4.2   \n",
            "3         0.2       278.5           0.8           9.4            7.1   \n",
            "4         0.5        41.3           1.2           4.2            1.4   \n",
            "\n",
            "   (BVOC)관측극초미세먼지  \n",
            "0             4.8  \n",
            "1             0.8  \n",
            "2             4.1  \n",
            "3             6.6  \n",
            "4             1.0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "데이터프레임에 결측치가 있습니다.\n",
            "각 열의 결측치 수:\n",
            "관측지점               0\n",
            "관측시간               0\n",
            "관측온도               8\n",
            "관측습도               8\n",
            "관측기압               8\n",
            "관측풍속              14\n",
            "관측풍향              14\n",
            "관측최대풍속             0\n",
            "관측미세먼지             0\n",
            "관측초미세먼지            0\n",
            "관측극초미세먼지           0\n",
            "배관관측온도             0\n",
            "(AVOC)관측온도         8\n",
            "(AVOC)관측습도         8\n",
            "(AVOC)관측기압         8\n",
            "(AVOC)관측풍속        12\n",
            "(AVOC)관측풍향        12\n",
            "(AVOC)관측최대풍속       0\n",
            "(AVOC)관측미세먼지       0\n",
            "(AVOC)관측초미세먼지      0\n",
            "(AVOC)관측극초미세먼지     0\n",
            "(AVOC)배관관측온도       0\n",
            "(BVOC)관측온도         8\n",
            "(BVOC)관측습도         8\n",
            "(BVOC)관측기압         8\n",
            "(BVOC)관측풍속        10\n",
            "(BVOC)관측풍향        10\n",
            "(BVOC)관측최대풍속       0\n",
            "(BVOC)관측미세먼지       0\n",
            "(BVOC)관측초미세먼지      0\n",
            "(BVOC)관측극초미세먼지     0\n",
            "관측지점세부             0\n",
            "dtype: int64\n",
            "훈련 데이터 크기: 409371\n",
            "테스트 데이터 크기: 45486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "id": "KTuYTmAbgrXg",
        "outputId": "3a9147bd-3c04-4714-d7c4-585aa9ef2595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dropout': 0.2, 'hidden_dim1': 64, 'hidden_dim2': 32, 'lr': 0.001, 'optimizer': 'Adam'}\n",
            "Epoch [1/10], Train Loss: 67.3659, Test Loss: 4.9126\n",
            "Epoch [2/10], Train Loss: 33.2454, Test Loss: 3.9765\n",
            "Epoch [3/10], Train Loss: 28.4124, Test Loss: 4.0379\n",
            "Epoch [4/10], Train Loss: 25.3389, Test Loss: 4.2396\n",
            "Epoch [5/10], Train Loss: 23.2486, Test Loss: 3.2164\n",
            "Epoch [6/10], Train Loss: 21.8508, Test Loss: 3.9468\n",
            "Epoch [7/10], Train Loss: 20.2646, Test Loss: 3.7486\n",
            "Epoch [8/10], Train Loss: 19.4115, Test Loss: 3.8549\n",
            "Epoch [9/10], Train Loss: 18.8197, Test Loss: 3.3144\n",
            "Epoch [10/10], Train Loss: 18.4726, Test Loss: 3.6820\n",
            "{'dropout': 0.2, 'hidden_dim1': 64, 'hidden_dim2': 32, 'lr': 0.001, 'optimizer': 'SGD'}\n",
            "Epoch [1/10], Train Loss: 708.0783, Test Loss: 736.1477\n",
            "Epoch [2/10], Train Loss: 736.3024, Test Loss: 736.1021\n",
            "Epoch [3/10], Train Loss: 736.0968, Test Loss: 736.1465\n",
            "Epoch [4/10], Train Loss: 736.0880, Test Loss: 736.0609\n",
            "Epoch [5/10], Train Loss: 736.0936, Test Loss: 736.0977\n",
            "Epoch [6/10], Train Loss: 788.9048, Test Loss: 736.0659\n",
            "Epoch [7/10], Train Loss: 735.9947, Test Loss: 736.0204\n",
            "Epoch [8/10], Train Loss: 735.9043, Test Loss: 736.0622\n",
            "Epoch [9/10], Train Loss: 736.0739, Test Loss: 736.0944\n",
            "Epoch [10/10], Train Loss: 736.0918, Test Loss: 736.1066\n",
            "{'dropout': 0.3, 'hidden_dim1': 64, 'hidden_dim2': 32, 'lr': 0.001, 'optimizer': 'Adam'}\n",
            "Epoch [1/10], Train Loss: 75.3726, Test Loss: 5.7954\n",
            "Epoch [2/10], Train Loss: 41.9967, Test Loss: 5.2663\n",
            "Epoch [3/10], Train Loss: 36.6260, Test Loss: 3.8960\n",
            "Epoch [4/10], Train Loss: 33.4230, Test Loss: 3.9563\n",
            "Epoch [5/10], Train Loss: 30.9872, Test Loss: 5.4824\n",
            "Epoch [6/10], Train Loss: 29.3076, Test Loss: 6.8822\n",
            "Epoch [7/10], Train Loss: 27.9320, Test Loss: 4.1330\n",
            "Epoch [8/10], Train Loss: 27.8770, Test Loss: 4.8087\n",
            "Epoch [9/10], Train Loss: 27.9129, Test Loss: 4.0738\n",
            "Epoch [10/10], Train Loss: 27.7100, Test Loss: 3.9626\n",
            "{'dropout': 0.3, 'hidden_dim1': 64, 'hidden_dim2': 32, 'lr': 0.001, 'optimizer': 'SGD'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-f1bee28641e0>\u001b[0m in \u001b[0;36m<cell line: 128>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# 그리드 서치 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mbest_result_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_result_r2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# 최적의 하이퍼파라미터 조합으로 모델 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-f1bee28641e0>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(model, train_data, train_label, dataloader, param_grid, input_dim, output_dim, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_vis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rmse'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/eval_vis.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(actual_values, predicted_values)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# RMSE 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# R-squared 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;36m0.825\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import eval_vis\n",
        "\n",
        "# CPU 혹은 GPU 사용, GPU 우선적으로 사용 가능\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 데이터 불러오기 (전처리 완료된 데이터라고 가정)\n",
        "train_data = pd.read_csv(\"/content/train_data.csv\")\n",
        "test_data = pd.read_csv(\"/content/test_data.csv\")\n",
        "\n",
        "# 특성과 타겟 변수 분리\n",
        "X_train = train_data.drop(columns=[\"관측미세먼지\"]).values\n",
        "X_test = test_data.drop(columns=[\"관측미세먼지\"]).values\n",
        "y_train = train_data[\"관측미세먼지\"].values\n",
        "y_test = test_data[\"관측미세먼지\"].values\n",
        "\n",
        "# 각각 train과 test를 알맞게 데이터와 매치, 관측 지점과 관측시간은 숫자형이 아니라서 일단 배제, 추후 필요 없는 특성이면 drop, 아니라면 다른 전처리 필요\n",
        "X_train = train_data.drop(columns=[\"관측미세먼지\", \"관측시간\"]).apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "X_test = test_data.drop(columns=[\"관측미세먼지\", \"관측시간\"]).apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "y_train = train_data[\"관측미세먼지\"].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "y_test = test_data[\"관측미세먼지\"].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "\n",
        "# 스탠다드 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Tensor로 변환\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# DataLoader 정의\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 모델 정의\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim1=64, hidden_dim2=32, dropout=0.2):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 그리드 서치 함수 정의\n",
        "def grid_search(model, train_data, train_label, dataloader, param_grid, input_dim, output_dim, num_epochs=15, batch_size=32):\n",
        "    results = []\n",
        "\n",
        "    param_list = list(ParameterGrid(param_grid))\n",
        "\n",
        "    for params in param_list:\n",
        "        model_1 = model(input_dim, output_dim, hidden_dim1=params['hidden_dim1'], hidden_dim2=params['hidden_dim2'], dropout=params['dropout']).to(device)\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = getattr(optim, params['optimizer'])(model_1.parameters(), lr=params['lr'])\n",
        "\n",
        "        print(params)\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model_1.train()\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model_1(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            train_losses.append(epoch_loss)\n",
        "\n",
        "            model_1.eval()\n",
        "            test_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                outputs = model_1(train_data.to(device))\n",
        "                test_loss = criterion(outputs, train_label.to(device)).item()\n",
        "                outputs = outputs.squeeze().cpu().numpy()\n",
        "                labels = train_label.cpu().numpy()\n",
        "                result = eval_vis.evaluate_model(labels, outputs)\n",
        "            test_losses.append(test_loss)\n",
        "            results.append({'params': params, 'rmse': result[0], 'r2': result[1]})\n",
        "\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    best_result_rmse = min(results, key=lambda x: x['rmse'])\n",
        "    best_result_r2 = max(results, key=lambda x: x['r2'])\n",
        "\n",
        "    print(\"Best Parameters for RMSE:\", best_result_rmse['params'])\n",
        "    print(\"Best RMSE:\", best_result_rmse['rmse'])\n",
        "\n",
        "    print(\"Best Parameters for R2:\", best_result_r2['params'])\n",
        "    print(\"Best R2:\", best_result_r2['r2'])\n",
        "\n",
        "    return best_result_rmse, best_result_r2\n",
        "\n",
        "# 하이퍼파라미터 그리드 정의\n",
        "param_grid = {\n",
        "    'hidden_dim1': [64],\n",
        "    'hidden_dim2': [32],\n",
        "    'dropout': [0.2 , 0.3],\n",
        "    'optimizer': ['Adam' , 'SGD'],\n",
        "    'lr': [0.001]\n",
        "}\n",
        "\n",
        "# 그리드 서치 수행\n",
        "best_result_rmse, best_result_r2 = grid_search(MLP, X_train, y_train, train_loader, param_grid, X_train.shape[1], 1, num_epochs=10, batch_size=32)\n",
        "\n",
        "# 최적의 하이퍼파라미터 조합으로 모델 학습\n",
        "best_params = best_result_rmse['params']\n",
        "model = MLP(X_train.shape[1], 1, hidden_dim1=best_params['hidden_dim1'], hidden_dim2=best_params['hidden_dim2'], dropout=best_params['dropout']).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = getattr(optim, best_params['optimizer'])(model.parameters(), lr=best_params['lr'])\n"
      ]
    }
  ]
}