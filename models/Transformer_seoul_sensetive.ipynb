{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A-TuvPrmXIEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa625c0-41b4-42c2-a3f6-749336037278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "!apt-get -qq install fonts-nanum\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib as mpl\n",
        "\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "fm.fontManager.addfont(fontpath)\n",
        "mpl.rc('font', family='NanumGothic')\n",
        "mpl.rcParams['axes.unicode_minus'] = False\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def evaluate_model(actual_values, predicted_values):\n",
        "\n",
        "    # RMSE 계산\n",
        "    rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))\n",
        "\n",
        "    # R-squared 계산\n",
        "    r2 = r2_score(actual_values, predicted_values)\n",
        "\n",
        "    return rmse, r2"
      ],
      "metadata": {
        "id": "CpBOE1SVY3IB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import recall_score, confusion_matrix\n",
        "\n",
        "# CPU 혹은 GPU 사용, GPU 우선적으로 사용 가능\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 데이터 불러오기 (전처리 완료된 데이터라고 가정)\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/train_seoul.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/test_seoul.csv\")\n",
        "\n",
        "# 특성과 타겟 변수 분리\n",
        "X_train = train_data.drop(columns=[\"관측미세먼지\"]).values\n",
        "X_test = test_data.drop(columns=[\"관측미세먼지\"]).values\n",
        "y_train = train_data[\"관측미세먼지\"].values\n",
        "y_test = test_data[\"관측미세먼지\"].values\n",
        "\n",
        "# 각각 train과 test를 알맞게 데이터와 매치, 관측 지점과 관측시간은 숫자형이 아니라서 일단 배제, 추후 필요 없는 특성이면 drop, 아니라면 다른 전처리 필요\n",
        "X_train = train_data.drop(columns=[\"관측미세먼지\", \"경과일\",\"경과시간\"]).apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "X_test = test_data.drop(columns=[\"관측미세먼지\",\"경과일\",\"경과시간\"]).apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "y_train = train_data[\"관측미세먼지\"].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "y_test = test_data[\"관측미세먼지\"].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "\n",
        "# 스탠다드 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Tensor로 변환\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# DataLoader 정의\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "MIGoHG1DXKAk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_encoder_layers=6, nhead=4, hidden_dim=512, dropout=0.1):\n",
        "        super(TransformerRegression, self).__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, input_dim)\n",
        "        x = self.input_proj(x)  # shape: (batch_size, seq_len, hidden_dim)\n",
        "        x = x.permute(1, 0, 2)  # shape: (seq_len, batch_size, hidden_dim)\n",
        "        x = self.transformer_encoder(x)  # shape: (seq_len, batch_size, hidden_dim)\n",
        "        x = x.mean(dim=0)  # shape: (batch_size, hidden_dim)\n",
        "        x = self.fc_out(x)  # shape: (batch_size, output_dim)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lZIMEFCjXQhN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그리드 서치 함수 정의\n",
        "def grid_search(model, train_data, train_label, dataloader, param_grid, input_dim, output_dim, num_epochs=15, batch_size=32):\n",
        "    results = []\n",
        "    param_list = list(ParameterGrid(param_grid))\n",
        "\n",
        "    for params in param_list:\n",
        "        model_1 = model(input_dim, output_dim, num_encoder_layers=params['num_encoder_layers'], hidden_dim=params['hidden_dim'], dropout=params['dropout']).to(device)\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = getattr(optim, params['optimizer'])(model_1.parameters(), lr=params['lr'])\n",
        "\n",
        "        print(params)\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model_1.train()\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                if inputs.dim() == 2:\n",
        "                    inputs = inputs.unsqueeze(1)  # (batch_size, seq_len, input_dim)으로 맞추기 위해 차원 추가\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model_1(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            train_losses.append(epoch_loss)\n",
        "\n",
        "            model_1.eval()\n",
        "            test_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                test_inputs = train_data.to(device)\n",
        "                if test_inputs.dim() == 2:\n",
        "                    test_inputs = test_inputs.unsqueeze(1)  # 테스트 데이터 차원도 확인\n",
        "                outputs = model_1(test_inputs)\n",
        "                test_loss = criterion(outputs, train_label.to(device)).item()\n",
        "                outputs = outputs.squeeze().cpu().numpy()\n",
        "                labels = train_label.cpu().numpy()\n",
        "                result = evaluate_model(labels, outputs)\n",
        "                sensitivity = recall_score((labels >= 81).astype(int), (outputs >= 81).astype(int))  # 민감도 계산 추가\n",
        "            test_losses.append(test_loss)\n",
        "            results.append({'params': params, 'rmse': result[0], 'r2': result[1], 'sensitivity': sensitivity})  # 민감도 결과 저장\n",
        "\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}, Sensitivity: {sensitivity:.4f}\")  # 민감도 출력\n",
        "\n",
        "    # 민감도가 가장 높은 모델의 파라미터 선택\n",
        "    best_result_sensitivity = max(results, key=lambda x: x['sensitivity'])  # 민감도가 가장 높은 모델 선택\n",
        "    best_result_rmse = min(results, key=lambda x: x['rmse'])\n",
        "    best_result_r2 = max(results, key=lambda x: x['r2'])\n",
        "\n",
        "    print(\"Best Parameters for Sensitivity:\", best_result_sensitivity['params'])  # 민감도가 가장 높은 모델의 파라미터 출력\n",
        "    print(\"Best Sensitivity:\", best_result_sensitivity['sensitivity'])  # 민감도가 가장 높은 모델의 민감도 출력\n",
        "\n",
        "    print(\"Best Parameters for RMSE:\", best_result_rmse['params'])\n",
        "    print(\"Best RMSE:\", best_result_rmse['rmse'])\n",
        "\n",
        "    print(\"Best Parameters for R2:\", best_result_r2['params'])\n",
        "    print(\"Best R2:\", best_result_r2['r2'])\n",
        "\n",
        "    return best_result_sensitivity, best_result_rmse, best_result_r2\n"
      ],
      "metadata": {
        "id": "XKkmWabaYgWA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 그리드 정의\n",
        "param_grid = {\n",
        "    'num_encoder_layers': [4],\n",
        "    'hidden_dim': [512],\n",
        "    'dropout': [0.1],\n",
        "    'lr': [0.01],\n",
        "    'optimizer': ['Adam']\n",
        "}"
      ],
      "metadata": {
        "id": "IszPFzTgY8zi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_result_sensitivity, best_result_rmse, best_result_r2 = grid_search(TransformerRegression, X_train, y_train, train_loader, param_grid, input_dim=28, output_dim=1, num_epochs=2, batch_size=32)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Best result by Sensitivity:\", best_result_sensitivity)\n",
        "print(\"Best result by RMSE:\", best_result_rmse)\n",
        "print(\"Best result by R2:\", best_result_r2)\n"
      ],
      "metadata": {
        "id": "_WNBv1OoZu1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ecfec7b-4aa2-471c-e06e-dae4bac348bf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dropout': 0.1, 'hidden_dim': 512, 'lr': 0.01, 'num_encoder_layers': 4, 'optimizer': 'Adam'}\n",
            "Epoch [1/2], Train Loss: 838.7515, Test Loss: 831.1143, Sensitivity: 0.0000\n",
            "Epoch [2/2], Train Loss: 834.1394, Test Loss: 829.5030, Sensitivity: 0.0000\n",
            "Best Parameters for Sensitivity: {'dropout': 0.1, 'hidden_dim': 512, 'lr': 0.01, 'num_encoder_layers': 4, 'optimizer': 'Adam'}\n",
            "Best Sensitivity: 0.0\n",
            "Best Parameters for RMSE: {'dropout': 0.1, 'hidden_dim': 512, 'lr': 0.01, 'num_encoder_layers': 4, 'optimizer': 'Adam'}\n",
            "Best RMSE: 28.801092\n",
            "Best Parameters for R2: {'dropout': 0.1, 'hidden_dim': 512, 'lr': 0.01, 'num_encoder_layers': 4, 'optimizer': 'Adam'}\n",
            "Best R2: -0.0030577166218106466\n",
            "Best result by Sensitivity: {'params': {'dropout': 0.1, 'hidden_dim': 512, 'lr': 0.01, 'num_encoder_layers': 4, 'optimizer': 'Adam'}, 'rmse': 28.829052, 'r2': -0.0050060961534383885, 'sensitivity': 0.0}\n",
            "Best result by RMSE: {'params': {'dropout': 0.1, 'hidden_dim': 512, 'lr': 0.01, 'num_encoder_layers': 4, 'optimizer': 'Adam'}, 'rmse': 28.801092, 'r2': -0.0030577166218106466, 'sensitivity': 0.0}\n",
            "Best result by R2: {'params': {'dropout': 0.1, 'hidden_dim': 512, 'lr': 0.01, 'num_encoder_layers': 4, 'optimizer': 'Adam'}, 'rmse': 28.801092, 'r2': -0.0030577166218106466, 'sensitivity': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "#민감도를 기준으로 최적의 파라미터 설정\n",
        "best_params = best_result_sensitivity['params']\n",
        "model = TransformerRegression(X_train.shape[1], 1, num_encoder_layers=best_params['num_encoder_layers'], nhead=4, hidden_dim=best_params['hidden_dim'], dropout=best_params['dropout']).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = getattr(optim, best_params['optimizer'])(model.parameters(), lr=best_params['lr'])\n",
        "\n",
        "# 학습\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        if inputs.dim() == 2:\n",
        "            inputs = inputs.unsqueeze(1)  # (batch_size, seq_len, input_dim)으로 맞추기 위해 차원 추가\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# 테스트 데이터에 대한 예측 및 민감도 계산\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test.to(device)).cpu().numpy()\n",
        "    y_pred = y_pred.squeeze()\n",
        "\n",
        "# 예측값을 고농도와 저농도로 분류 (81을 기준으로 분류)\n",
        "y_pred_classes = (y_pred >= 81).astype(int)\n",
        "y_test_classes = (y_test.squeeze().numpy() >= 81).astype(int)\n",
        "\n",
        "# 민감도 계산\n",
        "sensitivity_high = recall_score(y_test_classes, y_pred_classes, pos_label=1)\n",
        "sensitivity_low = recall_score(y_test_classes, y_pred_classes, pos_label=0)\n",
        "\n",
        "# 혼동 행렬을 통해 세부 통계 계산\n",
        "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "# 출력\n",
        "print(f'Total number of high concentration samples: {tp + fn}')\n",
        "print(f'Number of correctly identified high concentration samples: {tp}')\n",
        "print(f'Number of high concentration samples missed: {fn}')\n",
        "print(f'Sensitivity (Recall) for high concentration data: {sensitivity_high:.4f}')\n",
        "\n",
        "print(f'Total number of low concentration samples: {tn + fp}')\n",
        "print(f'Number of correctly identified low concentration samples: {tn}')\n",
        "print(f'Number of low concentration samples missed: {fp}')\n",
        "print(f'Sensitivity (Recall) for low concentration data: {sensitivity_low:.4f}')\n",
        "\n",
        "#loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test, label='Actual', color='blue')\n",
        "plt.plot(y_pred, label='Predicted', color='red', linestyle='--')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#시각화\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Low Concentration\", \"High Concentration\"], yticklabels=[\"Low Concentration\", \"High Concentration\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bdf0wplueg_B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "1bda3da5-0d89-4501-c711-8b62f93f4e47"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 838.8777\n",
            "Epoch [2/10], Loss: 833.8051\n",
            "Epoch [3/10], Loss: 834.4033\n",
            "Epoch [4/10], Loss: 833.4862\n",
            "Epoch [5/10], Loss: 832.6149\n",
            "Epoch [6/10], Loss: 832.8015\n",
            "Epoch [7/10], Loss: 832.7676\n",
            "Epoch [8/10], Loss: 831.9919\n",
            "Epoch [9/10], Loss: 832.5319\n",
            "Epoch [10/10], Loss: 832.0037\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0bec62cefb2a>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-329d7efc811d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# x shape: (batch_size, seq_len, input_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (batch_size, seq_len, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (seq_len, batch_size, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (seq_len, batch_size, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (batch_size, hidden_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
          ]
        }
      ]
    }
  ]
}